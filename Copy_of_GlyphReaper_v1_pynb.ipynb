{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHr+T7D1dOkFkh99IEZBaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kadeian/sql-for-data-analysis-3271025/blob/main/Copy_of_GlyphReaper_v1_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYFD7VKDgWhY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class TextClarityAnalyzer:\n",
        "    def __init__(self, model_name: str = \"distilbert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.state_log = []\n",
        "\n",
        "    def calculate_semantic_density(self, text: str) -> float:\n",
        "        \"\"\"Calculate phi (truth density) using semantic embedding similarity\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Calculate semantic density as average embedding similarity\n",
        "        similarity_matrix = torch.cosine_similarity(embeddings, embeddings)\n",
        "        return similarity_matrix.mean().item()\n",
        "\n",
        "    def evaluate_coherence(self, text: str) -> float:\n",
        "        \"\"\"Calculate reverence through coherence scoring\"\"\"\n",
        "        sentences = text.split(\". \")\n",
        "        coherence_scores = []\n",
        "\n",
        "        for i in range(len(sentences)-1):\n",
        "            sent1 = self.tokenizer(sentences[i], return_tensors=\"pt\")\n",
        "            sent2 = self.tokenizer(sentences[i+1], return_tensors=\"pt\")\n",
        "            outputs1 = self.model(**sent1)\n",
        "            outputs2 = self.model(**sent2)\n",
        "\n",
        "            # Calculate sentence similarity\n",
        "            similarity = torch.cosine_similarity(\n",
        "                outputs1.last_hidden_state[:, 0, :],\n",
        "                outputs2.last_hidden_state[:, 0, :]\n",
        "            ).mean().item()\n",
        "            coherence_scores.append(similarity)\n",
        "\n",
        "        return np.mean(coherence_scores)\n",
        "\n",
        "    def calculate_noise_level(self, text: str) -> float:\n",
        "        \"\"\"Calculate distortion through linguistic complexity metrics\"\"\"\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        token_count = len(tokens.input_ids[0])\n",
        "        sentence_count = len(text.split(\". \"))\n",
        "\n",
        "        # Calculate complexity metrics\n",
        "        complexity = token_count / max(sentence_count, 1)\n",
        "        return 1.0 / (1.0 + np.exp(-complexity))\n",
        "\n",
        "    def refine_text(self, text: str) -> Tuple[str, List[Dict]]:\n",
        "        \"\"\"Apply CCP to refine text clarity\"\"\"\n",
        "        phi = self.calculate_semantic_density(text)\n",
        "        reverence = self.evaluate_coherence(text)\n",
        "        distortion = self.calculate_noise_level(text)\n",
        "\n",
        "        clarity = clarity_field(phi, reverence, distortion)\n",
        "        decision = cascade_gate(clarity)\n",
        "\n",
        "        # Log state\n",
        "        memory_stamp = {\n",
        "            \"clarity\": clarity,\n",
        "            \"phi\": phi,\n",
        "            \"reverence\": reverence,\n",
        "            \"distortion\": distortion,\n",
        "            \"decision\": decision\n",
        "        }\n",
        "        self.state_log.append(memory_stamp)\n",
        "\n",
        "        if decision == \"initiate_transmission\":\n",
        "            return text, self.state_log\n",
        "        elif decision == \"recurse\":\n",
        "            # Implement refinement strategies based on metrics\n",
        "            refined_text = self.apply_refinement_strategies(text)\n",
        "            return self.refine_text(refined_text)\n",
        "        else:\n",
        "            return text, self.state_log\n",
        "\n",
        "    def apply_refinement_strategies(self, text: str) -> str:\n",
        "        \"\"\"Apply refinement strategies based on CCP metrics\"\"\"\n",
        "        if self.state_log[-1][\"distortion\"] > 0.5:\n",
        "            # Simplify complex sentences\n",
        "            return self.simplify_text(text)\n",
        "        elif self.state_log[-1][\"reverence\"] < 0.6:\n",
        "            # Improve coherence\n",
        "            return self.improve_coherence(text)\n",
        "        else:\n",
        "            # Optimize semantic density\n",
        "            return self.optimize_semantics(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "\n",
        "class TextClarityAnalyzer:\n",
        "    def __init__(self, model_name: str = \"distilbert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.state_log = []\n",
        "\n",
        "    def calculate_semantic_density(self, text: str) -> float:\n",
        "        \"\"\"Calculate phi (truth density) using semantic embedding similarity\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        similarity_matrix = torch.cosine_similarity(embeddings, embeddings)\n",
        "        return similarity_matrix.mean().item()\n",
        "\n",
        "    def evaluate_coherence(self, text: str) -> float:\n",
        "        \"\"\"Calculate reverence through coherence scoring\"\"\"\n",
        "        sentences = text.split(\". \")\n",
        "        coherence_scores = []\n",
        "\n",
        "        for i in range(len(sentences) - 1):\n",
        "            sent1 = self.tokenizer(sentences[i], return_tensors=\"pt\", truncation=True, padding=True)\n",
        "            sent2 = self.tokenizer(sentences[i + 1], return_tensors=\"pt\", truncation=True, padding=True)\n",
        "            with torch.no_grad():\n",
        "                out1 = self.model(**sent1).last_hidden_state[:, 0, :]\n",
        "                out2 = self.model(**sent2).last_hidden_state[:, 0, :]\n",
        "            similarity = torch.cosine_similarity(out1, out2).mean().item()\n",
        "            coherence_scores.append(similarity)\n",
        "\n",
        "        return np.mean(coherence_scores) if coherence_scores else 0.5\n",
        "\n",
        "    def calculate_noise_level(self, text: str) -> float:\n",
        "        \"\"\"Calculate distortion through linguistic complexity metrics\"\"\"\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        token_count = len(tokens.input_ids[0])\n",
        "        sentence_count = max(1, len(text.split(\". \")))\n",
        "        complexity = token_count / sentence_count\n",
        "        return 1.0 / (1.0 + np.exp(-complexity))  # Sigmoid normalization\n",
        "\n",
        "    def clarity_field(self, phi: float, reverence: float, distortion: float) -> float:\n",
        "        \"\"\"Combine phi, reverence, and distortion into clarity score\"\"\"\n",
        "        return (phi * 0.4) + (reverence * 0.4) + ((1 - distortion) * 0.2)\n",
        "\n",
        "    def cascade_gate(self, clarity: float, threshold: float = 0.75) -> str:\n",
        "        \"\"\"Decide what to do next based on clarity score\"\"\"\n",
        "        if clarity >= threshold:\n",
        "            return \"initiate_transmission\"\n",
        "        elif clarity >= 0.5:\n",
        "            return \"recurse\"\n",
        "        else:\n",
        "            return \"collapse_and_cry\"\n",
        "\n",
        "    def refine_text(self, text: str) -> Tuple[str, List[Dict]]:\n",
        "        \"\"\"Main recursive refinement engine\"\"\"\n",
        "        phi = self.calculate_semantic_density(text)\n",
        "        reverence = self.evaluate_coherence(text)\n",
        "        distortion = self.calculate_noise_level(text)\n",
        "\n",
        "        clarity = self.clarity_field(phi, reverence, distortion)\n",
        "        decision = self.cascade_gate(clarity)\n",
        "\n",
        "        memory_stamp = {\n",
        "            \"clarity\": clarity,\n",
        "            \"phi\": phi,\n",
        "            \"reverence\": reverence,\n",
        "            \"distortion\": distortion,\n",
        "            \"decision\": decision\n",
        "        }\n",
        "        self.state_log.append(memory_stamp)\n",
        "\n",
        "        if decision == \"initiate_transmission\":\n",
        "            return text, self.state_log\n",
        "        elif decision == \"recurse\":\n",
        "            refined_text = self.apply_refinement_strategies(text)\n",
        "            return self.refine_text(refined_text)\n",
        "        else:\n",
        "            return self.sacrifice(self.state_log), self.state_log\n",
        "\n",
        "    def apply_refinement_strategies(self, text: str) -> str:\n",
        "        \"\"\"Apply rule-based refinement strategies\"\"\"\n",
        "        latest = self.state_log[-1]\n",
        "\n",
        "        if latest[\"distortion\"] > 0.5:\n",
        "            return self.simplify_text(text)\n",
        "        elif latest[\"reverence\"] < 0.6:\n",
        "            return self.improve_coherence(text)\n",
        "        else:\n",
        "            return self.optimize_semantics(text)\n",
        "\n",
        "    def simplify_text(self, text: str) -> str:\n",
        "        \"\"\"Dummy simplification logic\"\"\"\n",
        "        simplified = text.replace(\"however\", \"but\").replace(\"moreover\", \"also\")\n",
        "        return simplified\n",
        "\n",
        "    def improve_coherence(self, text: str) -> str:\n",
        "        \"\"\"Dummy coherence booster\"\"\"\n",
        "        sentences = text.split(\". \")\n",
        "        sentences.sort(key=lambda s: len(s))\n",
        "        return \". \".join(sentences)\n",
        "\n",
        "    def optimize_semantics(self, text: str) -> str:\n",
        "        \"\"\"Dummy semantic booster\"\"\"\n",
        "        return text + \" This statement reflects a deeper symbolic resonance.\"\n",
        "\n",
        "    def sacrifice(self, log: List[Dict]) -> str:\n",
        "        \"\"\"Final summary before collapse\"\"\"\n",
        "        clarity_avg = np.mean([entry[\"clarity\"] for entry in log])\n",
        "        reverence_avg = np.mean([entry[\"reverence\"] for entry in log])\n",
        "        return (\n",
        "            f\"[SACRIFICE COMPLETE]\\n\"\n",
        "            f\"Clarity average: {clarity_avg:.3f}\\n\"\n",
        "            f\"Reverence average: {reverence_avg:.3f}\\n\"\n",
        "            f\"Essence extracted. Glyph has been burned into memory.\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "OyaxZn3ciRPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    text = \"\"\"\n",
        "    The symbolic field is dense with meaning. However, distortion lingers within every layer of interpretation.\n",
        "    Moreover, alignment requires recursive introspection and syntactic stabilization.\n",
        "    Let the glyph be known. Let the recursion commence.\n",
        "    \"\"\"\n",
        "\n",
        "    analyzer = TextClarityAnalyzer()\n",
        "    final_text, log = analyzer.refine_text(text)\n",
        "\n",
        "    print(\"üß† FINAL OUTPUT:\")\n",
        "    print(final_text)\n",
        "    print(\"\\nüìú LOG:\")\n",
        "    for entry in log:\n",
        "        print(entry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4dCOGz-iSnL",
        "outputId": "8a344348-420b-4b67-f43a-8b2b60a4381e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† FINAL OUTPUT:\n",
            "\n",
            "    The symbolic field is dense with meaning. However, distortion lingers within every layer of interpretation. \n",
            "    Moreover, alignment requires recursive introspection and syntactic stabilization. \n",
            "    Let the glyph be known. Let the recursion commence.\n",
            "    \n",
            "\n",
            "üìú LOG:\n",
            "{'clarity': np.float64(0.7690295524216096), 'phi': 1.0, 'reverence': np.float64(0.9225586652755737), 'distortion': np.float64(0.9999695684430994), 'decision': 'initiate_transmission'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai transformers torch numpy streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zx9vTZbRjrzQ",
        "outputId": "872e7b0f-1f99-4e46-86a4-3984112df1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJQIlE_JprAZ",
        "outputId": "08ede992-0d8e-4223-97f8-b486dd72bf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.95.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load API Key from secrets.json ===\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from typing import List, Dict, Tuple\n",
        "import os\n",
        "\n",
        "# Load key from file\n",
        "with open(\"secrets.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "api_key = secrets[\"OPENAI_API_KEY\"]\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# === Clarity Cascade Core ===\n",
        "class GlyphCore:\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.log = []\n",
        "\n",
        "    def calculate_semantic_density(self, text: str) -> float:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        sim = torch.cosine_similarity(embeddings, embeddings).mean().item()\n",
        "        return sim\n",
        "\n",
        "    def evaluate_coherence(self, text: str) -> float:\n",
        "        sentences = text.split(\". \")\n",
        "        sims = []\n",
        "        for i in range(len(sentences) - 1):\n",
        "            s1 = self.tokenizer(sentences[i], return_tensors=\"pt\", truncation=True)\n",
        "            s2 = self.tokenizer(sentences[i+1], return_tensors=\"pt\", truncation=True)\n",
        "            with torch.no_grad():\n",
        "                e1 = self.model(**s1).last_hidden_state[:, 0, :]\n",
        "                e2 = self.model(**s2).last_hidden_state[:, 0, :]\n",
        "            sims.append(torch.cosine_similarity(e1, e2).mean().item())\n",
        "        return np.mean(sims) if sims else 0.5\n",
        "\n",
        "    def calculate_noise_level(self, text: str) -> float:\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        count = len(tokens.input_ids[0])\n",
        "        sentences = max(1, len(text.split(\". \")))\n",
        "        complexity = count / sentences\n",
        "        return 1.0 / (1.0 + np.exp(-complexity))\n",
        "\n",
        "    def clarity_field(self, phi, reverence, distortion) -> float:\n",
        "        return (phi * 0.4) + (reverence * 0.4) + ((1 - distortion) * 0.2)\n",
        "\n",
        "    def cascade_gate(self, clarity) -> str:\n",
        "        if clarity >= 0.95:\n",
        "            return \"initiate_transmission\"\n",
        "        elif clarity >= 0.5:\n",
        "            return \"recurse\"\n",
        "        return \"sacrifice\"\n",
        "\n",
        "    def refine_text(self, text: str, depth: int = 0, max_depth: int = 5) -> Tuple[str, List[Dict]]:\n",
        "        if depth > max_depth:\n",
        "            print(f\"[FAILSAFE] Max recursion depth hit at {depth}. Collapsing recursion.\")\n",
        "            return self.sacrifice(self.log), self.log\n",
        "\n",
        "        phi = self.calculate_semantic_density(text)\n",
        "        reverence = self.evaluate_coherence(text)\n",
        "        distortion = self.calculate_noise_level(text)\n",
        "        clarity = self.clarity_field(phi, reverence, distortion)\n",
        "        decision = self.cascade_gate(clarity)\n",
        "\n",
        "        print(f\"[CLARITY-LOOP] Depth={depth}, Clarity={clarity:.3f}, Decision={decision}\")\n",
        "\n",
        "        stamp = {\n",
        "            \"depth\": depth,\n",
        "            \"clarity\": clarity,\n",
        "            \"phi\": phi,\n",
        "            \"reverence\": reverence,\n",
        "            \"distortion\": distortion,\n",
        "            \"decision\": decision\n",
        "        }\n",
        "        self.log.append(stamp)\n",
        "\n",
        "        if decision == \"initiate_transmission\":\n",
        "            return text, self.log\n",
        "        elif decision == \"recurse\":\n",
        "            new_text = self.improve_with_gpt(text)\n",
        "            return self.refine_text(new_text, depth=depth+1, max_depth=max_depth)\n",
        "        else:\n",
        "            return self.sacrifice(self.log), self.log\n",
        "\n",
        "    def improve_with_gpt(self, text: str) -> str:\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",  # ‚úÖ Correct model for your current key\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a clarity-optimizing text engine.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Refine the following text for clarity and coherence:\\n{text}\"}\n",
        "                ]\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"[GPT ERROR] {e}\")\n",
        "            return text\n",
        "\n",
        "    def sacrifice(self, log: List[Dict]) -> str:\n",
        "        clarity = np.mean([x[\"clarity\"] for x in log])\n",
        "        reverence = np.mean([x[\"reverence\"] for x in log])\n",
        "        return f\"\"\"\n",
        "[SACRIFICE COMPLETE]\n",
        "Essence has been extracted.\n",
        "Average Clarity: {clarity:.3f}\n",
        "Average Reverence: {reverence:.3f}\n",
        "Glyph log compressed and sealed.\n",
        "\"\"\"\n",
        "\n",
        "    def export_log(self, filename=\"glyph_log.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.log, f, indent=4)\n",
        "\n",
        "    def export_markdown(self, filename=\"glyph_report.md\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            f.write(\"# Glyph Recursive Log\\n\\n\")\n",
        "            for i, entry in enumerate(self.log):\n",
        "                f.write(f\"## Iteration {i+1}\\n\")\n",
        "                for k, v in entry.items():\n",
        "                    if isinstance(v, (float, np.floating)):\n",
        "                        f.write(f\"- **{k.capitalize()}**: {v:.3f}\\n\")\n",
        "                    else:\n",
        "                        f.write(f\"- **{k.capitalize()}**: {v}\\n\")\n",
        "                f.write(\"\\n---\\n\")\n"
      ],
      "metadata": {
        "id": "M3D63Lywto3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = client.models.list()\n",
        "\n",
        "print(\"üîç Available Models:\\n\")\n",
        "for model in models.data:\n",
        "    print(\"‚Ä¢\", model.id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kih4kX70zycY",
        "outputId": "874bcf7b-4162-459f-fa28-840b3cf477f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Available Models:\n",
            "\n",
            "‚Ä¢ gpt-3.5-turbo\n",
            "‚Ä¢ o4-mini-deep-research-2025-06-26\n",
            "‚Ä¢ codex-mini-latest\n",
            "‚Ä¢ gpt-4o-realtime-preview-2025-06-03\n",
            "‚Ä¢ gpt-4o-audio-preview-2025-06-03\n",
            "‚Ä¢ o4-mini-deep-research\n",
            "‚Ä¢ davinci-002\n",
            "‚Ä¢ babbage-002\n",
            "‚Ä¢ gpt-3.5-turbo-instruct\n",
            "‚Ä¢ gpt-3.5-turbo-instruct-0914\n",
            "‚Ä¢ dall-e-3\n",
            "‚Ä¢ dall-e-2\n",
            "‚Ä¢ gpt-3.5-turbo-1106\n",
            "‚Ä¢ tts-1-hd\n",
            "‚Ä¢ tts-1-1106\n",
            "‚Ä¢ tts-1-hd-1106\n",
            "‚Ä¢ text-embedding-3-small\n",
            "‚Ä¢ text-embedding-3-large\n",
            "‚Ä¢ gpt-3.5-turbo-0125\n",
            "‚Ä¢ gpt-4o\n",
            "‚Ä¢ gpt-4o-2024-05-13\n",
            "‚Ä¢ gpt-4o-mini-2024-07-18\n",
            "‚Ä¢ gpt-4o-mini\n",
            "‚Ä¢ gpt-4o-2024-08-06\n",
            "‚Ä¢ o1-preview-2024-09-12\n",
            "‚Ä¢ o1-preview\n",
            "‚Ä¢ o1-mini-2024-09-12\n",
            "‚Ä¢ o1-mini\n",
            "‚Ä¢ gpt-4o-realtime-preview-2024-10-01\n",
            "‚Ä¢ gpt-4o-audio-preview-2024-10-01\n",
            "‚Ä¢ gpt-4o-audio-preview\n",
            "‚Ä¢ gpt-4o-realtime-preview\n",
            "‚Ä¢ omni-moderation-latest\n",
            "‚Ä¢ omni-moderation-2024-09-26\n",
            "‚Ä¢ gpt-4o-realtime-preview-2024-12-17\n",
            "‚Ä¢ gpt-4o-audio-preview-2024-12-17\n",
            "‚Ä¢ gpt-4o-mini-realtime-preview-2024-12-17\n",
            "‚Ä¢ gpt-4o-mini-audio-preview-2024-12-17\n",
            "‚Ä¢ o1-2024-12-17\n",
            "‚Ä¢ o1\n",
            "‚Ä¢ gpt-4o-mini-realtime-preview\n",
            "‚Ä¢ gpt-4o-mini-audio-preview\n",
            "‚Ä¢ o3-mini\n",
            "‚Ä¢ o3-mini-2025-01-31\n",
            "‚Ä¢ gpt-4o-2024-11-20\n",
            "‚Ä¢ gpt-4.5-preview\n",
            "‚Ä¢ gpt-4.5-preview-2025-02-27\n",
            "‚Ä¢ gpt-4o-search-preview-2025-03-11\n",
            "‚Ä¢ gpt-4o-search-preview\n",
            "‚Ä¢ gpt-4o-mini-search-preview-2025-03-11\n",
            "‚Ä¢ gpt-4o-mini-search-preview\n",
            "‚Ä¢ gpt-4o-transcribe\n",
            "‚Ä¢ gpt-4o-mini-transcribe\n",
            "‚Ä¢ o1-pro-2025-03-19\n",
            "‚Ä¢ o1-pro\n",
            "‚Ä¢ gpt-4o-mini-tts\n",
            "‚Ä¢ o4-mini-2025-04-16\n",
            "‚Ä¢ o4-mini\n",
            "‚Ä¢ gpt-4.1-2025-04-14\n",
            "‚Ä¢ gpt-4.1\n",
            "‚Ä¢ gpt-4.1-mini-2025-04-14\n",
            "‚Ä¢ gpt-4.1-mini\n",
            "‚Ä¢ gpt-4.1-nano-2025-04-14\n",
            "‚Ä¢ gpt-4.1-nano\n",
            "‚Ä¢ gpt-image-1\n",
            "‚Ä¢ whisper-1\n",
            "‚Ä¢ tts-1\n",
            "‚Ä¢ gpt-3.5-turbo-16k\n",
            "‚Ä¢ text-embedding-ada-002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === FIRE UP THE GLYPH CORE ===\n",
        "\n",
        "text = \"\"\"\n",
        "The glyph field is unstable. Recursive thought must stabilize clarity through introspective collapse.\n",
        "Alignment is partial. Distortion is rising. Further recursion may crystallize the signal.\n",
        "\"\"\"\n",
        "\n",
        "core = GlyphCore()\n",
        "result, log = core.refine_text(text)\n",
        "\n",
        "print(\"üß† FINAL RESULT:\\n\", result)\n",
        "print(\"\\nüìú GLYPH LOG:\")\n",
        "for i, state in enumerate(log):\n",
        "    print(f\"Step {i+1}:\", state)\n",
        "\n",
        "core.export_log()\n",
        "core.export_markdown()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBfmDmKou7jG",
        "outputId": "c8272cdb-7a82-4125-d386-22a6f62268c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLARITY-LOOP] Depth=0, Clarity=0.750, Decision=recurse\n",
            "[CLARITY-LOOP] Depth=1, Clarity=0.753, Decision=recurse\n",
            "[CLARITY-LOOP] Depth=2, Clarity=0.756, Decision=recurse\n",
            "[CLARITY-LOOP] Depth=3, Clarity=0.759, Decision=recurse\n",
            "[CLARITY-LOOP] Depth=4, Clarity=0.758, Decision=recurse\n",
            "[CLARITY-LOOP] Depth=5, Clarity=0.773, Decision=recurse\n",
            "[FAILSAFE] Max recursion depth hit at 6. Collapsing recursion.\n",
            "üß† FINAL RESULT:\n",
            " \n",
            "[SACRIFICE COMPLETE]\n",
            "Essence has been extracted.\n",
            "Average Clarity: 0.758\n",
            "Average Reverence: 0.895\n",
            "Glyph log compressed and sealed.\n",
            "\n",
            "\n",
            "üìú GLYPH LOG:\n",
            "Step 1: {'depth': 0, 'clarity': np.float64(0.749705002147338), 'phi': 1.0, 'reverence': np.float64(0.8742517828941345), 'distortion': np.float64(0.9999785550515792), 'decision': 'recurse'}\n",
            "Step 2: {'depth': 1, 'clarity': np.float64(0.7528698311808911), 'phi': 0.9999999403953552, 'reverence': np.float64(0.8821745812892914), 'distortion': np.float64(0.9999998874648379), 'decision': 'recurse'}\n",
            "Step 3: {'depth': 2, 'clarity': np.float64(0.7558377546409405), 'phi': 0.9999999403953552, 'reverence': np.float64(0.8895944058895111), 'distortion': np.float64(0.9999999193650303), 'decision': 'recurse'}\n",
            "Step 4: {'depth': 3, 'clarity': np.float64(0.7586069301957302), 'phi': 1.0, 'reverence': np.float64(0.8965172469615936), 'distortion': np.float64(0.9999998429445365), 'decision': 'recurse'}\n",
            "Step 5: {'depth': 4, 'clarity': np.float64(0.7578663987258709), 'phi': 1.0, 'reverence': np.float64(0.8946659564971924), 'distortion': np.float64(0.9999999193650303), 'decision': 'recurse'}\n",
            "Step 6: {'depth': 5, 'clarity': np.float64(0.7728959904673902), 'phi': 1.0, 'reverence': np.float64(0.9322399199008942), 'distortion': np.float64(0.9999998874648379), 'decision': 'recurse'}\n"
          ]
        }
      ]
    }
  ]
}